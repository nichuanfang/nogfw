#!/usr/local/bin/python
# coding=utf-8
from time import sleep
import cv2
import requests
import logging
import io
from urllib import request, parse
import sys
import subprocess
from datetime import datetime
import os
import base64
import json
import yaml
import copy
import re
import qrcode
from PIL import Image
from qrcode import constants
# å›¾åƒè¯†åˆ«
import easyocr
# windowsä¸‹éœ€è¦å…ˆä¸‹è½½æ¨¡å‹æ–‡ä»¶  https://blog.csdn.net/Loliykon/article/details/114334699
reader = easyocr.Reader(['ch_sim','en'],model_storage_directory='ocr_models')

LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
DATE_FORMAT = "%m/%d/%Y %H:%M:%S %p"
logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, datefmt=DATE_FORMAT)

def qr_recognize(file_path:str):
    qrcode_filename = file_path
    qrcode_image = cv2.imread(qrcode_filename)
    qrCodeDetector = cv2.QRCodeDetector()
    data, bbox, straight_qrcode = qrCodeDetector.detectAndDecode(qrcode_image)
    return data

sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf-8')
raw_list = []
logging.basicConfig(level=logging.INFO)
def craw(number:int,video_id:str,sleeptime:int):
    # æœªå»é‡ æ‰“å¥½æ ‡ç­¾çš„èŠ‚ç‚¹åˆ—è¡¨
    all_nodes = []
    logging.info(f'===========================================================================å¼€å§‹è·å–èŠ‚ç‚¹ä¿¡æ¯...')
    count = 1
    # é»˜è®¤130
    for craw_index in range(number):
        logging.info(f'=====================================å¼€å§‹ç¬¬{craw_index+1}/{number}è½®æŠ“å–======================================================')
        # éš”ä¸€æ®µæ—¶é—´è·å–äºŒç»´ç 
        subprocess.call(f'ffmpeg -y -i "$(yt-dlp -g {video_id} | head -n 1)" -vframes 1 dist/last.jpg',shell=True)
        while True:
            if not os.path.exists('dist/last.jpg'):
                logging.info(f'==========================================================ç­‰å¾…æˆªå›¾ç”Ÿæˆ...======================================================')
                sleep(1)
            else:
                break
        try:
            logging.info(f'====================================={datetime.now().strftime("%Y-%m-%d %H:%M:%S")}--èŠ‚ç‚¹ä¿¡æ¯======================================================')
            # å¤„ç†ç”Ÿæˆçš„äºŒç»´ç  ç”ŸæˆèŠ‚ç‚¹ä¿¡æ¯
            data:str = qr_recognize(f'dist/last.jpg')
            raw_list.append(data)
            logging.info(f'===============================================================================raw_data: {data}')
            ocr_result = reader.readtext('dist/last.jpg')
            # additional handling to ocr result... 
            logging.info(f'===============================================================================OCR: {ocr_result}')
        except Exception as err:
            data = ''
            all_nodes = []
            logging.error(f'==============================={err}==============================================')
        sub_res = requests.get(f'https://sub.xeton.dev/sub?target=quanx&url={parse.quote(data)}&insert=false')
        sub_res_list: list[str] = sub_res.text.split('\n')
        for index,subitem in enumerate(sub_res_list):
            try:
                if subitem == '[server_local]' and sub_res_list[index+1] not in ['','[filter_local]']:
                    # æœ‰æ•ˆqxè®¢é˜…èŠ‚ç‚¹
                    # æ·»åŠ åˆ°ç›®æ ‡èŠ‚ç‚¹ä¸­
                    all_nodes.append(sub_res_list[index+1])
                    # èŠ‚ç‚¹å»é‡ åˆ©ç”¨å­—å…¸å»é‡
                    all_nodes = list(dict.fromkeys(all_nodes))
                    logging.info(f'==============================================================================å½“å‰èŠ‚ç‚¹æ± æœ‰: {len(all_nodes)}ä¸ªèŠ‚ç‚¹')
                    logging.info(f'')
                    logging.info(f'')
                    logging.info(f'')
                    logging.info(f'')
            except:
                continue
        if craw_index != number-1:
            sleep(sleeptime)
    return all_nodes

def resize(file):
    im = Image.open(file)
    reim=im.resize((640, 640))#å®½*é«˜

    reim.save(file,dpi=(300.0,300.0)) ##200.0,200.0åˆ†åˆ«ä¸ºæƒ³è¦è®¾å®šçš„dpiå€¼

def get_group_proxy_index(proxies:list):
    for index,proxy in enumerate(proxies):
        if proxy not in ['ğŸ”° èŠ‚ç‚¹é€‰æ‹©','â™»ï¸ è‡ªåŠ¨é€‰æ‹©','ğŸ¯ å…¨çƒç›´è¿']:
            return index
    return -1

def handle_group_proxy(final_dict,count,index):
    final_dict['proxy-groups'][index]['proxies'][get_group_proxy_index(final_dict['proxy-groups'][index]['proxies'])] = f'[{count}] '+final_dict['proxy-groups'][index] \
                    ['proxies'][get_group_proxy_index(final_dict['proxy-groups'][index]['proxies'])].replace('(Youtube:ä¸è‰¯æ—)','')
    
def filter_proxies(tag:str,proxies:list[str]):
    res = []
    for proxy in proxies:
        if tag == 'google':
            # ä½¿ç”¨å»¶è¿Ÿä½çš„èŠ‚ç‚¹ 
            if bool(re.search(r'é¦™æ¸¯|Hong Kong|HK|hk|æ–°åŠ å¡|Singapore|SG|sg|å°æ¹¾|Taiwan|TW|tw|å°åŒ—|æ—¥æœ¬|Japan|JP|jp|éŸ©å›½|Korea|KR|kr',proxy)):
                res.append(proxy)
        elif tag == 'github':
            # ä½¿ç”¨å»¶è¿Ÿä½çš„èŠ‚ç‚¹ 
            if bool(re.search(r'é¦™æ¸¯|Hong Kong|HK|hk|æ–°åŠ å¡|Singapore|SG|sg|å°æ¹¾|Taiwan|TW|tw|å°åŒ—|æ—¥æœ¬|Japan|JP|jp|éŸ©å›½|Korea|KR|kr',proxy)):
                res.append(proxy)
        elif tag == 'openai':
            # ä½¿ç”¨ç¾å›½èŠ‚ç‚¹ 
            if bool(re.search(r'ç¾å›½|United States|US|us',proxy)):
                res.append(proxy)
    # å¦‚æœæ²¡æœ‰å°±ç¼ºçœğŸ¯ å…¨çƒç›´è¿
    if len(res) == 0:
        res.append('ğŸ¯ å…¨çƒç›´è¿')
    return res

def direct_rulesets():
    unbreak_ruleset = requests.get('https://cdn.jsdelivr.net/gh/sve1r/Rules-For-Quantumult-X@develop/Rules/Services/Unbreak.list')
    china_ruleset = requests.get('https://raw.githubusercontent.com/sve1r/Rules-For-Quantumult-X/develop/Rules/Region/China.list')
    china_ip_ruleset = requests.get('https://raw.githubusercontent.com/sve1r/Rules-For-Quantumult-X/develop/Rules/Region/ChinaIP.list')

    unbreak_rules = unbreak_ruleset.text.split('\n')
    china_rules = china_ruleset.text.split('\n')
    china_ip_rules = china_ip_ruleset.text.split('\n')
    all_rules = unbreak_rules+china_rules+china_ip_rules
    final_rulesets = []
    for all_rule in all_rules:
        new_rule = all_rule.strip()
        if new_rule == '' or new_rule.startswith('#'):
            continue
        try:
            rule_list = new_rule.split(',')
        except:
            continue
        if len(rule_list) < 3:
            continue
        first = rule_list[0]
        second = rule_list[1]
        third = rule_list[2]
        if first == 'host' or first == 'HOST':
            if third == 'DIRECT' or third == 'direct':
                final_rulesets.append(','.join(['DOMAIN',second,'ğŸ¯ å…¨çƒç›´è¿']))

        elif first == 'host-suffix' or first == 'HOST-SUFFIX':
            if third == 'DIRECT' or third == 'direct':
                final_rulesets.append(','.join(['DOMAIN-SUFFIX','ğŸ¯ å…¨çƒç›´è¿']))

        elif first == 'host-keyword' or first == 'HOST-KEYWORD':
            final_rulesets.append(','.join(['DOMAIN-KEYWORD',second,'ğŸ¯ å…¨çƒç›´è¿']))

        elif first == 'ip-cidr' or first == 'IP-CIDR':
            final_rulesets.append(','.join(['IP-CIDR',second,'ğŸ¯ å…¨çƒç›´è¿']))

    return final_rulesets

def google_github_openai_ruleset():
    google_ruleset = requests.get('https://raw.githubusercontent.com/sve1r/Rules-For-Quantumult-X/develop/Rules/Services/Google.list')
    github_ruleset = requests.get('https://raw.githubusercontent.com/sve1r/Rules-For-Quantumult-X/develop/Rules/Services/Github.list')
    openai_ruleset = requests.get('https://raw.githubusercontent.com/sve1r/Rules-For-Quantumult-X/develop/Rules/Services/OpenAI.list')

    google_rules = google_ruleset.text.split('\n')
    github_rules = github_ruleset.text.split('\n')
    openai_rules = openai_ruleset.text.split('\n')
    all_rules = google_rules+github_rules+openai_rules

    final_rulesets = []
    for all_rule in all_rules:
        new_rule = all_rule.strip()
        if new_rule == '' or new_rule.startswith('#'):
            continue
        try:
            rule_list = new_rule.split(',')
        except:
            continue
        if len(rule_list) < 3:
            continue
        first = rule_list[0]
        second = rule_list[1]
        third = rule_list[2]
        if first == 'host' or first == 'HOST':
            if third == 'Google Domestic':
                final_rulesets.append(','.join(['DOMAIN',second,'ğŸ¯ å…¨çƒç›´è¿']))
            else:
                final_rulesets.append(','.join(['DOMAIN',second,third]))
        elif first == 'host-suffix' or first == 'HOST-SUFFIX':
            if third == 'Google Domestic':
                final_rulesets.append(','.join(['DOMAIN-SUFFIX','ğŸ¯ å…¨çƒç›´è¿']))
            else:
                final_rulesets.append(','.join(['DOMAIN-SUFFIX',second,third]))
        elif first == 'host-keyword' or first == 'HOST-KEYWORD':
            final_rulesets.append(','.join(['DOMAIN-KEYWORD',second,third]))
        elif first == 'ip-cidr' or first == 'IP-CIDR':
            final_rulesets.append(','.join(['IP-CIDR',second,third]))
    return final_rulesets

def generate_clash_config(raw_list:list,final_dict:dict): # type: ignore
    count = 1
    for index,raw in enumerate(raw_list):
        logging.info(f'handle raw:{raw}======================================')
        # sub_res = request.urlopen(f'https://sub.xeton.dev/sub?target=clash&url={parse.quote(raw)}&insert=false')
        sub_res = requests.get(f'https://sub.xeton.dev/sub?target=clash&url={parse.quote(raw)}&insert=false')
        # logging.info(f'è®¢é˜…è½¬æ¢åçš„å“åº”:çŠ¶æ€ç :{sub_res.status_code}  ok:{sub_res.ok}=====================================================')
        # logging.info(f'clash dict:{sub_res.text}======================================')
        if not sub_res.ok:
            continue
        try:
            data_dict:dict = yaml.load(sub_res.text, Loader=yaml.FullLoader)
            #logging.info(f'clash dict:{data_dict}======================================')
            if not final_dict:
                final_dict:dict = copy.deepcopy(data_dict)
                final_dict['socks-port'] = 10808 # type: ignore
                final_dict['port'] = 10809 # type: ignore
            #   #è‡ªåŠ¨é€‰æ‹© å¤šä¹…æ£€æµ‹ä¸€æ¬¡é€Ÿåº¦ è‡ªåŠ¨åˆ‡æ¢ å•ä½s(ç§’)
                final_dict['proxy-groups'][1]['interval'] = 600 # type: ignore
                # å‰”é™¤ä½å»¶è¿ŸèŠ‚ç‚¹
                if not bool(re.search(r'é¦™æ¸¯|Hong Kong|HK|hk|æ–°åŠ å¡|Singapore|SG|sg|å°æ¹¾|Taiwan|TW|tw|å°åŒ—|æ—¥æœ¬|Japan|JP|jp|éŸ©å›½|Korea|KR|kr',final_dict['proxy-groups'][1]['proxies'][0])):
                    final_dict['proxy-groups'][1]['proxies'] = []
                else:
                    # è‡ªåŠ¨é€‰æ‹©
                    handle_group_proxy(final_dict,count,1)
                proxy:dict= copy.deepcopy(data_dict['proxies'][0])
                final_dict['proxies'][0]['name'] = f'[{count}] ' + proxy['name'].replace('(Youtube:ä¸è‰¯æ—)','')
                # èŠ‚ç‚¹é€‰æ‹©
                handle_group_proxy(final_dict,count,0)
                # å›½å¤–åª’ä½“
                handle_group_proxy(final_dict,count,2)
                # å¾®è½¯æœåŠ¡
                handle_group_proxy(final_dict,count,4)
                # ç”µæŠ¥ä¿¡æ¯
                handle_group_proxy(final_dict,count,5)
                # è‹¹æœæœåŠ¡
                handle_group_proxy(final_dict,count,6)
                # æ¼ç½‘ä¹‹é±¼
                handle_group_proxy(final_dict,count,9)
                count+=1
            else:
                # æ·»åŠ èŠ‚ç‚¹
                proxy:dict= copy.deepcopy(data_dict['proxies'][0])

                proxy['name'] = f'[{count}] ' + proxy['name'].replace('(Youtube:ä¸è‰¯æ—)','')

                final_dict['proxies'].append(proxy)

                # åˆ†ç»„é…ç½®

                # èŠ‚ç‚¹é€‰æ‹©
                final_dict['proxy-groups'][0]['proxies'].append(proxy['name']) # type: ignore
                # è‡ªåŠ¨é€‰æ‹©
                # æ­£åˆ™åŒ¹é… æ’é™¤å»¶è¿Ÿä½çš„èŠ‚ç‚¹
                if bool(re.search(r'é¦™æ¸¯|Hong Kong|HK|hk|æ–°åŠ å¡|Singapore|SG|sg|å°æ¹¾|Taiwan|TW|tw|å°åŒ—|æ—¥æœ¬|Japan|JP|jp|éŸ©å›½|Korea|KR|kr',proxy['name'])):
                    final_dict['proxy-groups'][1]['proxies'].append(proxy['name']) # type: ignore
                # å›½å¤–åª’ä½“
                final_dict['proxy-groups'][2]['proxies'].append(proxy['name']) # type: ignore
                # å¾®è½¯æœåŠ¡
                final_dict['proxy-groups'][4]['proxies'].append(proxy['name']) # type: ignore
                # ç”µæŠ¥ä¿¡æ¯
                final_dict['proxy-groups'][5]['proxies'].append(proxy['name']) # type: ignore
                # è‹¹æœæœåŠ¡
                final_dict['proxy-groups'][6]['proxies'].append(proxy['name']) # type: ignore
                # æ¼ç½‘ä¹‹é±¼
                final_dict['proxy-groups'][9]['proxies'].append(proxy['name']) # type: ignore
                count+=1
        except Exception as e:
            logging.error(f'=========================================raw:{raw}è½¬æ¢ä¸ºclashé…ç½®æ–‡ä»¶å¤±è´¥!: {e}')
    if len(final_dict['proxy-groups'][1]['proxies'])==0:
        # å¦‚æœè‡ªåŠ¨é€‰æ‹©æ²¡ç”¨å¯ç”¨çš„èŠ‚ç‚¹ é»˜è®¤ğŸ¯ å…¨çƒç›´è¿ é˜²æ­¢clashå®¢æˆ·ç«¯æŠ¥é”™
        final_dict['proxy-groups'][1]['proxies'].append('ğŸ¯ å…¨çƒç›´è¿')
    proxies = []

    def sort_func(proxy):
        # è·å–æµ‹é€Ÿç»“æœ
        match = re.search(r'\d+.\d+',proxy.split('-')[-1])
        if match is not None:
            if proxy.split('-')[-1].lower().__contains__('mb'):
                return float(match.group())*1000
            return float(match.group())
        return 0.0
    
    for p in final_dict['proxies']:
        # æŒ‰ç…§æµ‹é€Ÿç»“æœæ’åº(é™åº) 
        proxies.append(p['name'])
    proxies.sort(key=sort_func,reverse=True) # type: ignore
    proxy_groups:list = final_dict['proxy-groups']
    # clashç­–ç•¥ç»„è¯¦ç»†é…ç½®è¯·æŸ¥çœ‹ https://stash.wiki/proxy-protocols/proxy-groups
    # æ·»åŠ è‡ªå®šä¹‰ç­–ç•¥ é«˜å¯ç”¨ Fallback
    proxy_groups.insert(2,{
        'name': 'ğŸ¤” é«˜å¯ç”¨',
        'type': 'fallback',
        'url': 'http://www.gstatic.com/generate_204',
        'interval': 43200,
        'proxies': proxies
    })
    final_dict['proxy-groups'][0]['proxies'].insert(1,'ğŸ¤” é«˜å¯ç”¨')
    # æ·»åŠ è‡ªå®šä¹‰ç­–ç•¥  Google
    proxy_groups.insert(3,{
        'name': 'Google',
        'type': 'fallback',
        'url': 'http://www.gstatic.com/generate_204',
        'interval': 43200,
        'proxies': filter_proxies('google',proxies)
    })
    # æ·»åŠ è‡ªå®šä¹‰ç­–ç•¥  Github
    proxy_groups.insert(4,{
        'name': 'Github',
        'type': 'fallback',
        'url': 'http://www.gstatic.com/generate_204',
        'interval': 43200,
        'proxies': filter_proxies('github',proxies)
    })

    # æ·»åŠ è‡ªå®šä¹‰ç­–ç•¥  OpenAI
    proxy_groups.insert(5,{
        'name': 'OpenAI',
        'type': 'fallback',
        'url': 'http://www.gstatic.com/generate_204',
        'interval': 43200,
        'proxies': filter_proxies('openai',proxies)
    })

    rules:list[str] = final_dict['rules']
    # æ·»åŠ è‡ªå®šä¹‰è§„åˆ™ åœ¨ç¬¬ä¸€ä¸ª`å›½å¤–åª’ä½“`ä¹‹å‰ æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
    logging.info(f'======================æ·»åŠ è‡ªå®šä¹‰è§„åˆ™: Google Github OpenAI==========================================')
    flag = 0
    for index,rule in enumerate(rules):
        if rule.__contains__('å›½å¤–åª’ä½“'):
            # æ‰¾åˆ°æ’å…¥ä½ç½®
            flag = index
            break
    rulesets = google_github_openai_ruleset()
    for rule_index,ruleset in enumerate(rulesets):
        rules.insert(flag+rule_index,ruleset)

    logging.info(f'======================æ·»åŠ è‡ªå®šä¹‰è§„åˆ™: ğŸ¯ å…¨çƒç›´è¿==========================================')
    # é’ˆå¯¹æ€§ç›´è¿
    
    for rule_ in rules:
        if rule_.__contains__('å…¨çƒç›´è¿'):
            try:
                rules.remove(rule_)
            except:
                continue
    logging.info(f'==========================================================æ·»åŠ è‡ªå®šä¹‰ç›´è¿ä¹‹å‰çš„rules: {rules}')
    direct_rules = direct_rulesets()
    for direct_rule in direct_rules:
        rules.append(direct_rule)
    return final_dict


if __name__ == '__main__':
    # ç¯å¢ƒ
    try:
        ENV = sys.argv[1]
    except:
        ENV = 'dev'
    if ENV == 'dev':
        CARW_NUMBER = 5
        NEED_SAVE = False
    elif ENV == 'prod':
        CARW_NUMBER = 150
        NEED_SAVE = True
    else:
        CARW_NUMBER = 5
        NEED_SAVE = False

    # sys.argv[1]): CRAW_NUMBER æŠ“å–æ¬¡æ•°
    all_nodes = craw(CARW_NUMBER,'qmRkvKo-KbQ',10)
    # å¯¹èŠ‚ç‚¹æŒ‰ç…§æµ‹é€Ÿç»“æœ ä»å¿«åˆ°æ…¢é™é€Ÿæ’åº
    def qx_sort(node):
        # è·å–æµ‹é€Ÿç»“æœ
        match = re.search(r'\d+.\d+',node.split('-')[-1])
        if match is not None:
            if node.split('-')[-1].lower().__contains__('mb'):
                return float(match.group())*1000
            return float(match.group())
        return 0.0
    all_nodes.sort(key=qx_sort,reverse=True) # type: ignore
    # sorted_nodes = sort_nodes(all_nodes)
    taged_nodes = []
    # èŠ‚ç‚¹æ›´æ”¹tag
    for index,node in enumerate(all_nodes):
        new_node = None
        # æ›´æ”¹tag
        match = re.search(r'tag.+$',node)
        if match is not None:
            tag = match.group()
            new_tag = 'tag='+f'[{index+1}] '+tag.replace('(Youtube:ä¸è‰¯æ—)','').split('=')[1]
            new_node = re.sub(r'tag.+$',new_tag,node)
        if new_node == None:
            continue
        taged_nodes.append(new_node)
    
    # ç”Ÿæˆqxä¸“ç”¨è®¢é˜…
    if NEED_SAVE:
        open('dist/qx-sub','w+').write('\n'.join(taged_nodes))

    # ç”Ÿæˆclashé…ç½®æ–‡ä»¶
    logging.info(f'=========================================================================ç”Ÿæˆclashé…ç½®æ–‡ä»¶...')

    # raw_list = ['vmess://eyJ2IjoiMiIsInBzIjoi576O5Zu9LTUuNjNNQi9zKFlvdXR1YmU65LiN6Imv5p6XKSIsImFkZCI6IjIzLjIyNC4xMTAuMTg0IiwicG9ydCI6IjQ0MyIsInR5cGUiOiJub25lIiwiaWQiOiI0MTgwNDhhZi1hMjkzLTRiOTktOWIwYy05OGNhMzU4MGRkMjQiLCJhaWQiOiI2NCIsIm5ldCI6IndzIiwicGF0aCI6Ii9wYXRoLzA4MDcxMjM0MjMxMCIsImhvc3QiOiIiLCJ0bHMiOiJ0bHMifQ==','vmess://eyJ2IjoiMiIsInBzIjoi576O5Zu9LTQuMzlNQi9zKFlvdXR1YmU65LiN6Imv5p6XKSIsImFkZCI6IjE5OC4yLjE5Ni40OSIsInBvcnQiOiI1NDQzNCIsInR5cGUiOiJub25lIiwiaWQiOiI0MTgwNDhhZi1hMjkzLTRiOTktOWIwYy05OGNhMzU4MGRkMjQiLCJhaWQiOiI2NCIsIm5ldCI6InRjcCIsInBhdGgiOiIvIiwiaG9zdCI6IiIsInRscyI6IiJ9']

    # rawæ•°æ®å»é‡
    raw_list = copy.deepcopy(sorted(set(raw_list),key=raw_list.index))
    # base64åŠ å¯†
    encoder = base64.b64encode(('\n'.join(raw_list)).encode("utf-8"))
    # è§£ç ä¸º utf-8 å­—ç¬¦ä¸²
    try:
        # ç”Ÿæˆé€šç”¨è®¢é˜…
        if NEED_SAVE:
            open('dist/sub', 'w+',encoding='utf-8').write(encoder.decode('utf-8'))
    except Exception as e:
        logging.error(f'================================é€šç”¨è®¢é˜…ç”Ÿæˆå¤±è´¥!:{e}==========================================')

    # ç”Ÿæˆé€šç”¨è®¢é˜…äºŒç»´ç 
    try:
        qr = qrcode.QRCode(version=40
                    ,error_correction=constants.ERROR_CORRECT_M,
                    box_size=15, border=4,
                    image_factory=None,
                    mask_pattern=None)
        # è‡ªé€‚åº”å¤§å°
        if not NEED_SAVE:
            qr.add_data('\n'.join(raw_list))
            img = qr.make_image()
            with open('dist/sub.jpg', 'wb') as qrc:
                img.save(qrc)
            # è°ƒæ•´åˆ†è¾¨ç‡
            resize('dist/sub.jpg')
    except Exception as e:
        logging.error(f'================================äºŒç»´ç ç”Ÿæˆå¤±è´¥!:{e}==========================================')
    
    try:
        clash_dict = generate_clash_config(raw_list,{})
        with open('dist/clash.yml', 'w+',encoding='utf-8') as file:
            if NEED_SAVE:
                file.write(yaml.dump(clash_dict, allow_unicode=True,default_flow_style=False,sort_keys=False))
    except Exception as e:
        logging.error(f'================================clashæ–‡ä»¶ç”Ÿæˆå¤±è´¥!:{e}==========================================')

    logging.info(f'=========================================================================clashé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ!')
    logging.info(f'')
    logging.info(f'')
    logging.info(f'')
    logging.info(f'')
    logging.info(f'=========================================================================èŠ‚ç‚¹æ›´æ–°å®Œæˆ!')
